{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as torch_split\n",
    "import numpy as np\n",
    "import dataset\n",
    "import test\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler\n",
    "from monai.losses import DiceLoss\n",
    "from monai.losses import FocalLoss\n",
    "from torchmetrics.classification import F1Score\n",
    "from monai.networks.nets import UNet\n",
    "from monai.data import DataLoader\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall\n",
    "import sys\n",
    "sys.path.insert(1, 'H:/Projects/Kaggle/CZII-CryoET-Object-Identification/preprocessing')\n",
    "import visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"H:/Projects/Kaggle/CZII-CryoET-Object-Identification/datasets/3D/dim104-3000sample\"\n",
    "data = dataset.UNetDataset(path=path)\n",
    "\n",
    "tv_split = 0.8\n",
    "trn = int(len(data) * tv_split)\n",
    "val = len(data) - trn\n",
    "\n",
    "train_dataset, val_dataset = torch_split.random_split(data, [trn, val])\n",
    "# train_dataset = dataset.UNetDataset(path=path, train=True)\n",
    "# val_dataset = dataset.UNetDataset(path=path, val=True)\n",
    "labels = [\n",
    "\"background\",\n",
    "\"apo-ferritin(E)\",\n",
    "\"beta-amylase(NS)\",\n",
    "\"beta-galactosidase(H)\",\n",
    "\"ribosome(E)\",\n",
    "\"thyroglobulin(H)\",\n",
    "\"virus-like-particle(E)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import UNet\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vis.new_trial()\n",
    "    \n",
    "    warmup_epochs = 0\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    decay = trial.suggest_float('decay', 0.3, 1.0)\n",
    "    # dropout = trial.suggest_float(\"dropout\", 0.25, 0.5)\n",
    "    dropout = 0.3\n",
    "    regularization_strength = trial.suggest_float(\"regularization_strength\", 5e-5, 1e-2, log=True)\n",
    "    # alpha = trial.suggest_float(\"alpha\", 0.25, 1.0)\n",
    "    # theta = trial.suggest_float(\"theta\", 0.1, 0.9)\n",
    "    theta = 0.6\n",
    "    gamma = trial.suggest_float(\"gamma\", 2.0, 5.0)\n",
    "    \n",
    "    # t_max = trial.suggest_int(\"t_max\", np.ceil(0.3 * cosine_epochs), np.ceil(0.5 * cosine_epochs))\n",
    "    \n",
    "    # regularization_type = trial.suggest_categorical(\"regularization_type\", [\"none\", \"L1\", \"L2\"])\n",
    "    # Suggest regularization strength only if regularization is used\n",
    "    # if regularization_type != \"none\":\n",
    "    #     regularization_strength = trial.suggest_float(\"regularization_strength\", 1e-5, 1e-3, log=True)\n",
    "    # else:\n",
    "    #     regularization_strength = 0\n",
    "\n",
    "    # Model initialization\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=7,\n",
    "        channels=(64, 128, 256, 512),\n",
    "        strides=(2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    \n",
    "    weights = torch.tensor([0.0434743, 1.16546, 1.1661, 1.16513, 1.14281, 1.15554, 1.16149]).to(device)  # Example weights for classes\n",
    "\n",
    "    dice_loss = DiceLoss(to_onehot_y=True, softmax=True, weight=weights).to(device)\n",
    "    focal_loss = FocalLoss(to_onehot_y=True, use_softmax=True, weight=torch.tensor([0.0] + list(weights[1:])), gamma=gamma ).to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Learning rate warmup (LinearLR) and then cosine annealing (CosineAnnealingLR)\n",
    "    \n",
    "    scheduler_warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=decay)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Regularization setup\n",
    "    def add_regularization_loss(model, regularization_type, regularization_strength):\n",
    "        reg_loss = 0\n",
    "        if regularization_type == \"L1\":\n",
    "            for param in model.parameters():\n",
    "                reg_loss += torch.sum(torch.abs(param))\n",
    "        elif regularization_type == \"L2\":\n",
    "            for param in model.parameters():\n",
    "                reg_loss += torch.sum(param ** 2)\n",
    "        return regularization_strength * reg_loss\n",
    "\n",
    "    # num_epochs = warmup_epochs + cosine_epochs\n",
    "    num_epochs = 25\n",
    "    prec, rec = 0, 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch['src'].float().to(device), batch['tgt'].long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = (theta) * dice_loss(outputs, targets) + (1 - theta) * focal_loss(outputs, targets)\n",
    "\n",
    "            # Add regularization loss if applicable\n",
    "            # if regularization_type != \"none\":\n",
    "            reg_loss = add_regularization_loss(model, \"L2\", regularization_strength)\n",
    "            loss += reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Scheduler step after each epoch\n",
    "        if epoch < warmup_epochs:\n",
    "            scheduler_warmup.step()\n",
    "        else:\n",
    "            # scheduler_cosine.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        precision_metric = MulticlassPrecision(num_classes=7, average='none').to(device)\n",
    "        recall_metric = MulticlassRecall(num_classes=7, average='none').to(device)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch['src'].float().to(device), batch['tgt'].long().to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = (theta) * dice_loss(outputs, targets) + (1 - theta) * focal_loss(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                precision_metric.update(outputs.argmax(dim=1).flatten(), targets.flatten())\n",
    "                recall_metric.update(outputs.argmax(dim=1).flatten(), targets.flatten())\n",
    "        # print(\"batch done\")\n",
    "        val_loss /= len(val_loader)\n",
    "        precision = precision_metric.compute().cpu()\n",
    "        recall = recall_metric.compute().cpu()\n",
    "        pr = torch.stack([precision, recall], dim=0)\n",
    "        vis.report(val_loss, pr)\n",
    "        \n",
    "        class_weights = np.array([0.00621062, 0.16649395, 0.16658561, 0.16644739, 0.16325901, 0.16507644, 0.16592698])\n",
    "        \n",
    "        # print(precision.shape)\n",
    "        # print(recall.shape)\n",
    "        precision = precision.detach().numpy()\n",
    "        recall = recall.detach().numpy()\n",
    "        \n",
    "        weighted_prec = np.sum(precision * class_weights)\n",
    "        weighted_rec = np.sum(recall * class_weights)\n",
    "        \n",
    "        obj = val_loss / 0.6 + (1 - (0.75 * weighted_rec + 0.25 * weighted_prec) )\n",
    "        \n",
    "        # Report weighted loss between loss, prec, recall results to Optuna\n",
    "        trial.report(obj, epoch)\n",
    "\n",
    "        # Prune trial if necessary\n",
    "        if trial.should_prune():\n",
    "            \n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # print(f\"Final validation loss: {val_loss:.6g}\")\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "n_epochs = 25\n",
    "vis = visual.loss_precision_recall(n_epochs, labels, 1.2)\n",
    "vis.start()\n",
    "\n",
    "# Run the Optuna optimization with Median Pruner\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=MedianPruner(n_startup_trials=4, n_warmup_steps=8))\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"param_search_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vis.data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tbackground   |   apo-ferritin(E)   |   beta-amylase(NS)   |   beta-galactosidase(H)   |   ribosome(E)   |   thyroglobulin(H)   |   virus-like-particle(E)   |   \n",
      "Trial 0 loss 0.575\n",
      "\t\t\t0.998\t|\t   0.001\t|\t   0.000\t|\t   0.000\t|\t   0.187\t|\t   0.021\t|\t   0.024\t|\t   \n",
      "\t\t\t0.312\t|\t   0.030\t|\t   0.041\t|\t   0.012\t|\t   0.961\t|\t   0.537\t|\t   0.439\t|\t   \n",
      "Trial 1 loss 0.591\n",
      "\t\t\t0.980\t|\t   0.000\t|\t   0.000\t|\t   0.001\t|\t   0.068\t|\t   0.006\t|\t   0.003\t|\t   \n",
      "\t\t\t0.109\t|\t   0.024\t|\t   0.138\t|\t   0.077\t|\t   0.674\t|\t   0.073\t|\t   0.090\t|\t   \n",
      "Trial 2 loss 0.577\n",
      "\t\t\t0.996\t|\t   0.000\t|\t   0.000\t|\t   0.001\t|\t   0.180\t|\t   0.040\t|\t   0.022\t|\t   \n",
      "\t\t\t0.189\t|\t   0.060\t|\t   0.031\t|\t   0.051\t|\t   0.922\t|\t   0.582\t|\t   0.377\t|\t   \n",
      "Trial 3 loss 0.429\n",
      "\t\t\t0.991\t|\t   0.482\t|\t   0.000\t|\t   0.277\t|\t   0.637\t|\t   0.361\t|\t   0.693\t|\t   \n",
      "\t\t\t0.489\t|\t   0.792\t|\t   0.393\t|\t   0.542\t|\t   0.812\t|\t   0.599\t|\t   0.874\t|\t   \n",
      "Trial 4 loss 0.586\n",
      "\t\t\t0.993\t|\t   0.000\t|\t   0.000\t|\t   0.001\t|\t   0.112\t|\t   0.021\t|\t   0.001\t|\t   \n",
      "\t\t\t0.233\t|\t   0.034\t|\t   0.115\t|\t   0.080\t|\t   0.869\t|\t   0.263\t|\t   0.036\t|\t   \n",
      "Trial 5 loss 0.411\n",
      "\t\t\t0.992\t|\t   0.554\t|\t   0.206\t|\t   0.383\t|\t   0.611\t|\t   0.331\t|\t   0.679\t|\t   \n",
      "\t\t\t0.975\t|\t   0.816\t|\t   0.463\t|\t   0.532\t|\t   0.791\t|\t   0.640\t|\t   0.858\t|\t   \n",
      "Trial 6 loss 0.406\n",
      "\t\t\t0.992\t|\t   0.670\t|\t   0.197\t|\t   0.322\t|\t   0.633\t|\t   0.397\t|\t   0.744\t|\t   \n",
      "\t\t\t0.979\t|\t   0.730\t|\t   0.415\t|\t   0.577\t|\t   0.820\t|\t   0.620\t|\t   0.896\t|\t   \n",
      "Trial 7 loss 0.460\n",
      "\t\t\t0.992\t|\t   0.003\t|\t   0.000\t|\t   0.309\t|\t   0.652\t|\t   0.392\t|\t   0.781\t|\t   \n",
      "\t\t\t0.487\t|\t   0.680\t|\t   0.191\t|\t   0.542\t|\t   0.855\t|\t   0.527\t|\t   0.882\t|\t   \n",
      "Trial 8 loss 0.468\n",
      "\t\t\t0.992\t|\t   0.250\t|\t   0.003\t|\t   0.001\t|\t   0.503\t|\t   0.360\t|\t   0.700\t|\t   \n",
      "\t\t\t0.486\t|\t   0.553\t|\t   0.018\t|\t   0.363\t|\t   0.888\t|\t   0.554\t|\t   0.853\t|\t   \n"
     ]
    }
   ],
   "source": [
    "with open(\"param_search_results.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "print(\"\\t\\t\\t\", end=\"\")\n",
    "for label in labels: print(f\"{label}   |   \", end=\"\")\n",
    "print()\n",
    "for trial in range(9):\n",
    "    epochs = len(data[\"losses\"][trial])\n",
    "    print(f\"Trial {trial} loss {data['losses'][trial][epochs-1]:.3f}\")\n",
    "    print(\"\\t\\t\\t\", end=\"\")\n",
    "    for i in range(7):\n",
    "        print(f\"{data['label_pr'][trial][i][0][epochs-1].item():.3f}\", end=\"\\t|\\t   \")\n",
    "    print()\n",
    "    print(\"\\t\\t\\t\",end=\"\")\n",
    "    for i in range(7):\n",
    "        print(f\"{data['label_pr'][trial][i][1][epochs-1].item():.3f}\", end=\"\\t|\\t   \")\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKING TRAIL 6: lr 0.002378749151980637 decay 0.6383495211595349 regularization_strength 0.0014973716879159318 gamma 3.461783291951009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'losses': [],   trials x epochs (per trial)\n",
    "# 'label_pr': []  trials x labels x 2 (p & r) x epochs\n",
    "\n",
    "# [I 2024-12-24 13:38:05,114] A new study created in memory with name: no-name-3d26efe3-d09b-41e6-9859-6e780bb52cf4\n",
    "# [I 2024-12-24 14:39:11,594] Trial 0 finished with value: 0.575233225759707 and parameters: {'lr': 8.895982362405156e-05, 'decay': 0.4973760593275419, 'regularization_strength': 0.0025172052804279532, 'gamma': 4.996031337088363}. Best is trial 0 with value: 0.575233225759707.\n",
    "# [I 2024-12-24 15:37:22,894] Trial 1 finished with value: 0.591125853751835 and parameters: {'lr': 1.2990040043526965e-05, 'decay': 0.6813198310736835, 'regularization_strength': 0.0024965757066016288, 'gamma': 4.369881718243024}. Best is trial 0 with value: 0.575233225759707.\n",
    "# [I 2024-12-24 16:35:33,212] Trial 2 finished with value: 0.5770129514367957 and parameters: {'lr': 4.4440170274243755e-05, 'decay': 0.31695145632882915, 'regularization_strength': 8.74331159347397e-05, 'gamma': 4.2913558753361665}. Best is trial 0 with value: 0.575233225759707.\n",
    "# [I 2024-12-24 17:49:47,555] Trial 3 finished with value: 0.42899969769151586 and parameters: {'lr': 0.0007172248192508939, 'decay': 0.8544127180269929, 'regularization_strength': 0.0051075629067560855, 'gamma': 4.62092221228817}. Best is trial 3 with value: 0.42899969769151586.\n",
    "# [I 2024-12-24 18:25:46,168] Trial 4 pruned. \n",
    "# [I 2024-12-24 19:45:32,846] Trial 5 finished with value: 0.41076544240901347 and parameters: {'lr': 0.009262442723082055, 'decay': 0.8135586839953626, 'regularization_strength': 0.0001678451998607637, 'gamma': 3.6492772738208084}. Best is trial 5 with value: 0.41076544240901347.\n",
    "# [I 2024-12-24 20:43:49,142] Trial 6 finished with value: 0.4059352302237561 and parameters: {'lr': 0.002378749151980637, 'decay': 0.6383495211595349, 'regularization_strength': 0.0014973716879159318, 'gamma': 3.461783291951009}. Best is trial 6 with value: 0.4059352302237561.\n",
    "# [I 2024-12-24 21:41:53,587] Trial 7 finished with value: 0.460155421181729 and parameters: {'lr': 0.0011120877059222305, 'decay': 0.7041753205677597, 'regularization_strength': 0.00032375245652757075, 'gamma': 3.589166472143006}. Best is trial 6 with value: 0.4059352302237561.\n",
    "# [W 2024-12-24 22:18:59,469] Trial 8 failed with parameters: {'lr': 0.0006692026907199952, 'decay': 0.6625227751501456, 'regularization_strength': 0.00014299575697903301, 'gamma': 4.666629649772913} because of the following error: KeyboardInterrupt().\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
